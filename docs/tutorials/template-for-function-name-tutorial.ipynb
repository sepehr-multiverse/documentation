{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6d1e3ec",
   "metadata": {},
   "source": [
    "{/* cspell:ignore interpretability */}\n",
    "# Hybrid Quantum-Enhanced Ensemble Classification (Grid Stability Workflow)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6f69b77",
   "metadata": {},
   "source": [
    "*Usage estimate: 12 minutes on ibm_brisbane. (NOTE: This is an estimate only. Your runtime may vary.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf80006",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "This tutorial introduces a hybrid quantum–classical workflow designed to demonstrate how quantum optimization can enhance ensemble learning performance in real-world machine learning tasks. Using the *Singularity Machine Learning – Classification* function by Multiverse Computing, users will perform a complete end-to-end workflow for grid stability prediction, comparing classical ensemble baselines against a quantum-optimized alternative. The underlying technology combines classical ensemble techniques such as boosting and bagging with quantum algorithms like the Quantum Approximate Optimization Algorithm (QAOA), which optimizes the diversity and generalization of the trained ensemble. Unlike traditional quantum models limited by the number of qubits or dataset size, this function allows large-scale datasets with millions of examples to be processed efficiently, as the number of qubits only constrains the ensemble size. Through Qiskit Serverless, this tutorial shows how to connect, configure, and execute these hybrid workflows seamlessly on IBM’s managed quantum infrastructure—either through simulators or real QPUs—giving users both practical experience and an understanding of where quantum optimization can provide measurable benefits in classical learning pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b94021",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "Before starting this tutorial, ensure you have the following packages installed in your Python environment:\n",
    "\n",
    "- Qiskit SDK v2.1.0 or later, with visualization support\n",
    "- Qiskit Serverless v0.24.0 or later\n",
    "- Qiskit IBM Catalog v0.8.0 or later\n",
    "- scikit-learn v1.5.2\n",
    "- pandas v2.x\n",
    "- imbalanced-learn v0.12.3\n",
    "- qiskit-ibm-runtime v0.40.1\n",
    "\n",
    "You can install all required dependencies by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00f85769-0c6c-40c9-bb27-622d95e353fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q \"qiskit[visualization]~=2.1.0\" \"qiskit-serverless~=0.24.0\" \"qiskit-ibm-runtime~=0.40.1\" \"qiskit-ibm-catalog~=0.8.0\" \"scikit-learn==1.5.2\" \"pandas>=2.0.0,<3.0.0\" \"imbalanced-learn~=0.12.3\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7db2e559",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "In this section, we initialize the Qiskit Serverless client and load the **Singularity Machine Learning – Classification** function provided by Multiverse Computing.\n",
    "Qiskit Serverless allows you to run hybrid quantum–classical workflows on IBM’s managed cloud infrastructure without worrying about resource management.\n",
    "You will need an **IBM Quantum Platform API key** and your **cloud resource name (CRN)** to authenticate and access Qiskit Functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fe7ee1-21ce-445c-b151-598cd4cf9227",
   "metadata": {},
   "source": [
    "### Download the dataset\n",
    "\n",
    "To run this tutorial, we use a preprocessed **grid stability classification dataset** containing power system sensor readings labeled as *stable* or *unstable*.  \n",
    "The dataset is publicly hosted in the Multiverse Computing fork of the Qiskit documentation repository.  \n",
    "The following cell automatically creates the required folder structure and downloads both the training and test files directly into your environment using `wget`.  \n",
    "If you already have these files locally, this step will safely overwrite them to ensure version consistency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a32efb3-a425-4c02-804b-65029ecffb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_tutorial/grid_ 100%[===================>] 612.94K  --.-KB/s    in 0.03s   \n",
      "data_tutorial/grid_ 100%[===================>] 108.19K  --.-KB/s    in 0.008s  \n",
      "Dataset files downloaded:\n",
      "-rw-r--r--@ 1 sepehr.hosseini  staff   108K 23 Oct 12:29 data_tutorial/grid_stability/test.csv\n",
      "-rw-r--r--@ 1 sepehr.hosseini  staff   613K 23 Oct 12:29 data_tutorial/grid_stability/train.csv\n"
     ]
    }
   ],
   "source": [
    "## Download dataset for Grid Stability Classification\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "!mkdir -p data_tutorial/grid_stability\n",
    "\n",
    "# Download the training and test sets from the official Qiskit documentation repo\n",
    "!wget -q --show-progress -O data_tutorial/grid_stability/train.csv \\\n",
    "  https://raw.githubusercontent.com/sepehr-multiverse/documentation/mvc-sml/docs/tutorials/data/grid_stability/train.csv\n",
    "\n",
    "!wget -q --show-progress -O data_tutorial/grid_stability/test.csv \\\n",
    "  https://raw.githubusercontent.com/sepehr-multiverse/documentation/mvc-sml/docs/tutorials/data/grid_stability/test.csv\n",
    "\n",
    "# Check the files have been downloaded\n",
    "!echo \"Dataset files downloaded:\"\n",
    "!ls -lh data_tutorial/grid_stability/*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9aa84f-ab37-412c-b056-7043b73380fa",
   "metadata": {},
   "source": [
    "### Import required packages\n",
    "\n",
    "In this section, we import all Python packages and Qiskit modules used throughout the tutorial.  \n",
    "These include core scientific libraries for data handling and model evaluation—such as `NumPy`, `pandas`, and `scikit-learn`—along with visualization tools and IBM Qiskit components for running the quantum-enhanced model.  \n",
    "We also import the `QiskitRuntimeService` and `QiskitFunctionsCatalog` to connect with IBM Quantum services and access the **Singularity Machine Learning** function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8c654f5-8355-4f67-b79d-c2b1c29ccc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit_ibm_runtime import QiskitRuntimeService\n",
    "from qiskit_ibm_catalog import QiskitFunctionsCatalog\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import time\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4709dc1c-b380-49f1-95c7-89197aa5e147",
   "metadata": {},
   "source": [
    "### Connect to IBM Quantum and load the Singularity Function\n",
    "\n",
    "Next, we authenticate with IBM Quantum services and load the **Singularity Machine Learning – Classification** function from the Qiskit Functions Catalog.  \n",
    "The `QiskitRuntimeService` establishes a secure connection to the IBM Quantum Platform using your API token and instance CRN, allowing access to quantum backends.  \n",
    "The `QiskitFunctionsCatalog` is then used to retrieve the Singularity function by name (`\"multiverse/singularity\"`), enabling us to call it later for hybrid quantum–classical computation.  \n",
    "If the setup is successful, you will see a confirmation message indicating that the function has been loaded correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc380c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to IBM Qiskit Serverless and loaded the Singularity function.\n"
     ]
    }
   ],
   "source": [
    "IBM_TOKEN = \"\"\n",
    "IBM_INSTANCE_TEST = \"\"\n",
    "IBM_INSTANCE_QUANTUM = \"\"\n",
    "FUNCTION_NAME = \"multiverse/singularity\"\n",
    "\n",
    "service = QiskitRuntimeService(\n",
    "    token=IBM_TOKEN, channel=\"ibm_quantum_platform\", instance=IBM_INSTANCE_QUANTUM\n",
    ")\n",
    "\n",
    "backend = service.least_busy()\n",
    "catalog = QiskitFunctionsCatalog(\n",
    "    token=IBM_TOKEN, instance=IBM_INSTANCE_TEST, channel=\"ibm_quantum_platform\"\n",
    ")\n",
    "singularity = catalog.load(FUNCTION_NAME)\n",
    "print(\n",
    "    \"Successfully connected to IBM Qiskit Serverless and loaded the Singularity function.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d6a559-118a-4aa9-874d-9c009b5da60c",
   "metadata": {},
   "source": [
    "### Define helper functions\n",
    "\n",
    "Before running the main experiments, we define a few small utility functions that streamline data loading and model evaluation.  \n",
    "- `load_data()` reads the input CSV files into NumPy arrays, splitting features and labels for compatibility with scikit-learn and quantum workflows.  \n",
    "- `evaluate_predictions()` computes key performance metrics—accuracy, precision, recall, and F1-score—and optionally reports runtime if timing information is provided.  \n",
    "\n",
    "These helper functions simplify repeated operations later in the notebook and ensure consistent metric reporting across both classical and quantum classifiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46bc841e-7365-4508-b6bf-ae57db6050e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Load data from the given path to X and y arrays.\"\"\"\n",
    "    df: pd.DataFrame = pd.read_csv(data_path)\n",
    "    return df.iloc[:, :-1].values, df.iloc[:, -1].values\n",
    "\n",
    "\n",
    "def evaluate_predictions(predictions, y_true, start_time=None, end_time=None):\n",
    "    accuracy = accuracy_score(y_true, predictions)\n",
    "    precision = precision_score(y_true, predictions)\n",
    "    recall = recall_score(y_true, predictions)\n",
    "    f1 = f1_score(y_true, predictions)\n",
    "    if start_time is not None and end_time is not None:\n",
    "        print(\"Time taken (s):\", end_time - start_time)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1:\", f1)\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "988ee237",
   "metadata": {},
   "source": [
    "## Step 1: Prepare and balance the dataset\n",
    "\n",
    "We begin by loading the grid stability dataset and preparing it for model training and evaluation.  \n",
    "The data is split into training, validation, and test subsets using an 80/20 split for validation.  \n",
    "Because real-world grid stability data is often imbalanced—with significantly more stable than unstable samples—we apply **random over-sampling** using `imbalanced-learn` to balance the training set.  \n",
    "This ensures that both the classical AdaBoost baseline and the quantum-enhanced ensemble classifier learn from a representative distribution of classes.  \n",
    "\n",
    "At the end of this step, the code prints the shape of each data split to verify that the data has been correctly loaded and resampled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0db0e914-a8f2-4a04-bec7-c15bac8104b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: X_train_bal (5104, 12) y_train_bal (5104,) X_val (850, 12) y_val (850,) X_test (750, 12) y_test (750,)\n"
     ]
    }
   ],
   "source": [
    "RANDOM_STATE: int = 123\n",
    "TRAIN_PATH = \"data_tutorial/grid_stability/train.csv\"\n",
    "TEST_PATH = \"data_tutorial/grid_stability/test.csv\"\n",
    "# Load and upload the data\n",
    "X_train, y_train = load_data(TRAIN_PATH)\n",
    "X_test, y_test = load_data(TEST_PATH)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Balance the dataset through over-sampling of the positive class\n",
    "ros = RandomOverSampler(random_state=RANDOM_STATE)\n",
    "X_train_bal, y_train_bal = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\n",
    "    \"Shapes:\",\n",
    "    \"X_train_bal\",\n",
    "    X_train_bal.shape,\n",
    "    \"y_train_bal\",\n",
    "    y_train_bal.shape,\n",
    "    \"X_val\",\n",
    "    X_val.shape,\n",
    "    \"y_val\",\n",
    "    y_val.shape,\n",
    "    \"X_test\",\n",
    "    X_test.shape,\n",
    "    \"y_test\",\n",
    "    y_test.shape,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6f36e3",
   "metadata": {},
   "source": [
    "## Step 2: Optimize problem for quantum hardware execution\n",
    "\n",
    "The ensemble selection task is cast as a combinatorial optimization problem where each weak learner is a binary decision variable and the objective balances accuracy with sparsity through a regularization term. The `QuantumEnhancedEnsembleClassifier` solves this with QAOA on IBM hardware, while still allowing simulator-based exploration. In this tutorial we configure a realistic production profile that scales to larger ensembles: we use 75 learners to mirror a strong classical baseline, set the regularization to favor compact yet accurate ensembles, and select a QAOA configuration that converges reliably within a practical time budget. The `optimizer_options` control the hybrid loop: `simulator=False` routes circuits to the selected QPU, `num_solutions` increases search breadth, and `classical_optimizer_options` (for the inner classical optimizer) govern convergence; values around 60 iterations are a good balance for quality and runtime. Runtime options such as moderate circuit depth (`reps`) and a standard transpilation effort help ensure robust performance across devices. The configuration below is the “best-results” profile we will use for hardware runs; you may also create a purely simulated variant by toggling `simulator=True` to dry-run the workflow without consuming QPU time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18c0d3f9-691b-449d-83ca-e6bfce2d6b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured hardware optimization profile: \n",
      "  learners=75 \n",
      "  regularization=15 \n",
      "  optimizer_options={'simulator': False, 'num_solutions': 100000, 'reps': 3, 'optimization_level': 3, 'num_transpiler_runs': 30, 'classical_optimizer': 'COBYLA', 'classical_optimizer_options': {'maxiter': 10}, 'estimator_options': None, 'sampler_options': None}\n"
     ]
    }
   ],
   "source": [
    "# Problem scale and regularization\n",
    "NUM_LEARNERS = 75  # ensemble size (also drives qubit count)\n",
    "REGULARIZATION = 15  # favors smaller, less complex ensembles\n",
    "\n",
    "# QAOA / runtime configuration for best results on hardware\n",
    "optimizer_options = {\n",
    "    \"simulator\": False,  # set True to test locally without QPU\n",
    "    \"num_solutions\": 100_000,  # broaden search over candidate ensembles\n",
    "    \"reps\": 3,  # QAOA depth (circuit layers)\n",
    "    \"optimization_level\": 3,  # transpilation effort\n",
    "    \"num_transpiler_runs\": 30,  # explore multiple layouts\n",
    "    \"classical_optimizer\": \"COBYLA\",  # robust default for this landscape\n",
    "    \"classical_optimizer_options\": {\n",
    "        \"maxiter\": 10  # practical convergence budget\n",
    "    },\n",
    "    # You can pass backend-specific options; leaving None uses least-busy routing\n",
    "    \"estimator_options\": None,\n",
    "    \"sampler_options\": None,\n",
    "}\n",
    "\n",
    "print(\n",
    "    \"Configured hardware optimization profile:\",\n",
    "    f\"\\n  learners={NUM_LEARNERS}\",\n",
    "    f\"\\n  regularization={REGULARIZATION}\",\n",
    "    f\"\\n  optimizer_options={optimizer_options}\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4d480b3",
   "metadata": {},
   "source": [
    "## Step 3: Execute using Qiskit primitives\n",
    "\n",
    "We now execute the full workflow and compare a strong classical baseline with the quantum-optimized ensemble. First, we train an AdaBoost classifier with an ensemble size comparable to the quantum configuration to establish a reproducible reference on the test set. Next, we invoke the Singularity Function’s `create_fit_predict` action to build, optimize, and evaluate the `QuantumEnhancedEnsembleClassifier` end-to-end on IBM infrastructure. The job returns predictions together with status and message fields that confirm successful execution. For convenience and reproducibility, we keep the classical train/validation split from Step 1 and pass the validation data into `fit_params` so the quantum optimization can use it internally for model selection without touching the held-out test set. After both runs complete, we compute accuracy, precision, recall, and F1 to enable a direct, task-level comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6efe0c8-4836-4a92-b1de-9c12ae22f0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sepehr.hosseini/Documents/Qiskit/TUTORIAL (NOT LAB)/.venv/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classical AdaBoost baseline:\n",
      "Accuracy: 0.7853333333333333\n",
      "Precision: 1.0\n",
      "Recall: 0.7853333333333333\n",
      "F1: 0.8797610156833457\n",
      "\n",
      "-- Submitting quantum-enhanced ensemble job --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sepehr.hosseini/Documents/Qiskit/TUTORIAL (NOT LAB)/.venv/lib/python3.12/site-packages/qiskit_ibm_runtime/utils/json.py:385: UserWarning: Callable <IBMBackend('ibm_kingston')> is not JSON serializable and will be set to None.\n",
      "  warnings.warn(f\"Callable {obj} is not JSON serializable and will be set to None.\")\n"
     ]
    }
   ],
   "source": [
    "# ----- Classical baseline: AdaBoost -----\n",
    "baseline = AdaBoostClassifier(n_estimators=75, random_state=RANDOM_STATE)\n",
    "baseline.fit(X_train_bal, y_train_bal)\n",
    "baseline_pred = baseline.predict(X_test)\n",
    "\n",
    "print(\"Classical AdaBoost baseline:\")\n",
    "_ = evaluate_predictions(baseline_pred, y_test)\n",
    "\n",
    "# ----- Quantum-enhanced ensemble on IBM hardware -----\n",
    "print(\"\\n-- Submitting quantum-enhanced ensemble job --\")\n",
    "start = time.time()\n",
    "job = singularity.run(\n",
    "    action=\"create_fit_predict\",\n",
    "    name=\"grid_stability_qeec\",\n",
    "    quantum_classifier=\"QuantumEnhancedEnsembleClassifier\",\n",
    "    num_learners=NUM_LEARNERS,\n",
    "    regularization=REGULARIZATION,\n",
    "    optimizer_options=optimizer_options,  # from Step 2\n",
    "    backend_name=backend,  # least-busy compatible backend\n",
    "    instance=IBM_INSTANCE_QUANTUM,\n",
    "    random_state=RANDOM_STATE,\n",
    "    X_train=X_train_bal,\n",
    "    y_train=y_train_bal,\n",
    "    X_test=X_test,\n",
    "    fit_params={\"validation_data\": (X_val, y_val)},\n",
    "    options={\"save\": False},\n",
    ")\n",
    "\n",
    "# Retrieve result and evaluate\n",
    "status = job.status()\n",
    "result = job.result()\n",
    "end = time.time()\n",
    "\n",
    "print(\"\\nQuantum job status:\", status)\n",
    "print(\"Action status:\", result.get(\"status\"))\n",
    "print(\"Action message:\", result.get(\"message\"))\n",
    "\n",
    "qeec_pred = np.array(result[\"data\"][\"predictions\"])\n",
    "print(\"\\nQuantum-enhanced ensemble results:\")\n",
    "_ = evaluate_predictions(qeec_pred, y_test, start, end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b94af2",
   "metadata": {},
   "source": [
    "## Step 4: Post-process and return result in desired classical format\n",
    "\n",
    "After both executions finish, we summarize performance with standard classification metrics and a compact visualization. We compute accuracy, precision, recall, and F1 for the classical baseline and the quantum-enhanced ensemble on the same held-out test set, assemble the results into a small table for easy inspection, and render a simple bar chart comparing F1 scores. This concise post-processing highlights task-level differences without exposing the validation split, ensuring a clean, reproducible comparison between classical and quantum optimization paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902bec0b-9fce-446f-b888-eddf5e64a039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recompute metrics to ensure synchronized reporting\n",
    "def metrics_dict(y_true, y_pred, label):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    rec = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    return {\n",
    "        \"Model\": label,\n",
    "        \"Accuracy\": acc,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": rec,\n",
    "        \"F1\": f1,\n",
    "    }\n",
    "\n",
    "\n",
    "summary = pd.DataFrame(\n",
    "    [\n",
    "        metrics_dict(y_test, baseline_pred, \"AdaBoost (Classical)\"),\n",
    "        metrics_dict(y_test, qeec_pred, \"Quantum-Enhanced Ensemble\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Evaluation summary:\")\n",
    "display(\n",
    "    summary.style.format(\n",
    "        {\n",
    "            \"Accuracy\": \"{:.4f}\",\n",
    "            \"Precision\": \"{:.4f}\",\n",
    "            \"Recall\": \"{:.4f}\",\n",
    "            \"F1\": \"{:.4f}\",\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "# Bar chart comparing F1\n",
    "plt.figure(figsize=(7, 4.5))\n",
    "plt.bar(summary[\"Model\"], summary[\"F1\"])\n",
    "plt.ylabel(\"F1 score\")\n",
    "plt.title(\"F1 comparison: Classical vs Quantum-enhanced\")\n",
    "for i, v in enumerate(summary[\"F1\"]):\n",
    "    plt.text(i, v + 0.01, f\"{v:.3f}\", ha=\"center\", va=\"bottom\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.xticks(rotation=10)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56da023c",
   "metadata": {},
   "source": [
    "## Appendix: Scaling Benefits and Enhancements\n",
    "\n",
    "The scalability advantage of the `QuantumEnhancedEnsembleClassifier` arises from how the ensemble-selection process maps to quantum optimization.\n",
    "Classical ensemble learning methods, such as AdaBoost or random forests, become computationally expensive as the number of weak learners increases because selecting the optimal subset is a combinatorial problem that scales exponentially.\n",
    "\n",
    "In contrast, the quantum formulation—implemented here via the Quantum Approximate Optimization Algorithm (QAOA)—can explore these exponentially large search spaces more efficiently by evaluating multiple configurations in superposition.\n",
    "As a result, the training time grows with the number of learners, allowing the model to remain efficient even at larger ensemble scales.\n",
    "\n",
    "While current hardware introduces some noise and depth limitations, this workflow demonstrates a near-term hybrid approach where classical and quantum components cooperate: the quantum optimizer provides a better initialization landscape for the classical loop, improving both convergence and final model accuracy.\n",
    "As quantum processors evolve, these scaling benefits are expected to extend to larger datasets, more learners, and deeper circuit depths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee41a301",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [\\[1\\]](#Reference1) [Qiskit Function Tutorial](https://docs.quantum.ibm.com/guides/functions)  \n",
    "- [\\[2\\]](#Reference3) [Multiverse Computing – Singularity Machine Learning Function Documentation](https://quantum.cloud.ibm.com/docs/en/guides/multiverse-computing-singularity)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb5785c",
   "metadata": {},
   "source": [
    "## Tutorial survey\n",
    "\n",
    "Please take a minute to provide feedback on this tutorial. Your insights will help us improve our content offerings and user experience.\n",
    "\n",
    "[Link to survey](https://your.feedback.ibm.com/jfe/form/SV_3BLFkNVEuh0QBWm)"
   ]
  }
 ],
 "metadata": {
  "description": "TODO - description for SEO between 50 and 160 characters",
  "kernelspec": {
   "display_name": "qiskit-functions",
   "language": "python",
   "name": "qiskit-functions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "title": "TODO - The title of your tutorial"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
